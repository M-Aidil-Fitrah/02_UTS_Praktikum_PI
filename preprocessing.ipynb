{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f46f0006",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß© CSV diperbaiki ‚Üí dataset/etd_ugm_fixed_fixed.csv (8578 data)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8578/8578 [00:00<00:00, 124907.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ dataset/etd_ugm_fixed.csv selesai diproses ‚Üí clean_dataset/etd_ugm_fixed.csv\n",
      "üß© CSV diperbaiki ‚Üí dataset/etd_usk_fixed_fixed.csv (9796 data)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9796/9796 [00:00<00:00, 130969.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ dataset/etd_usk_fixed.csv selesai diproses ‚Üí clean_dataset/etd_usk_fixed.csv\n",
      "üß© CSV diperbaiki ‚Üí dataset/kompas_fixed_fixed.csv (10000 data)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10000/10000 [00:00<00:00, 206939.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ dataset/kompas_fixed.csv selesai diproses ‚Üí clean_dataset/kompas_fixed.csv\n",
      "üß© CSV diperbaiki ‚Üí dataset/mojok_fixed_fixed.csv (9484 data)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9484/9484 [00:00<00:00, 192030.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ dataset/mojok_fixed.csv selesai diproses ‚Üí clean_dataset/mojok_fixed.csv\n",
      "üß© CSV diperbaiki ‚Üí dataset/tempo_fixed_fixed.csv (7560 data)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7560/7560 [00:00<00:00, 184094.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ dataset/tempo_fixed.csv selesai diproses ‚Üí clean_dataset/tempo_fixed.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "import csv\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "\n",
    "# ========== OPSIONAL: progress bar ==========\n",
    "# Hapus 3 baris ini setelah selesai pengujian\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "# ============================================\n",
    "\n",
    "\n",
    "# ======== 0. Fungsi Perbaikan CSV Tidak Rapi ========\n",
    "\n",
    "def perbaiki_csv_tidak_rapi(input_path, output_path):\n",
    "    \"\"\"\n",
    "    Gabungkan baris CSV yang kontennya terpotong oleh newline di dalam tanda kutip.\n",
    "    Misal:\n",
    "    Judul,\"Baris pertama\n",
    "    baris kedua\"\n",
    "    ‚Üí jadi satu baris utuh.\n",
    "    \"\"\"\n",
    "    with open(input_path, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    hasil = []\n",
    "    buffer = ''\n",
    "    judul = ''\n",
    "\n",
    "    for line in lines:\n",
    "        line = line.strip('\\n')\n",
    "        if not line:\n",
    "            continue\n",
    "\n",
    "        # Jika line mengandung kutip pembuka tapi belum kutip penutup\n",
    "        if line.count('\"') == 1 and buffer == '':\n",
    "            if ',' in line:\n",
    "                judul, buffer = line.split(',', 1)\n",
    "            else:\n",
    "                continue\n",
    "            continue\n",
    "\n",
    "        # Jika sedang menggabung isi konten\n",
    "        if buffer:\n",
    "            buffer += ' ' + line\n",
    "            if buffer.count('\"') % 2 == 0:\n",
    "                hasil.append([judul.strip(), buffer.strip('\"').strip()])\n",
    "                buffer = ''\n",
    "            continue\n",
    "\n",
    "        # Baris normal (judul + konten sudah lengkap)\n",
    "        if line.count('\"') >= 2 and ',' in line:\n",
    "            judul, konten = line.split(',', 1)\n",
    "            hasil.append([judul.strip(), konten.strip('\"').strip()])\n",
    "\n",
    "    # Simpan hasil yang sudah diperbaiki\n",
    "    with open(output_path, 'w', encoding='utf-8', newline='') as f_out:\n",
    "        writer = csv.writer(f_out)\n",
    "        writer.writerow(['judul', 'konten'])\n",
    "        writer.writerows(hasil)\n",
    "\n",
    "    print(f\"üß© CSV diperbaiki ‚Üí {output_path} ({len(hasil)} data)\")\n",
    "\n",
    "\n",
    "# ======== 1. Fungsi Preprocessing ========\n",
    "\n",
    "def casefolding(text):\n",
    "    return text.lower()\n",
    "\n",
    "def normalisasi(text):\n",
    "    text = re.sub(r'[^a-z0-9\\s]', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "def tokenisasi(text):\n",
    "    return text.split()\n",
    "\n",
    "def filter_token(tokens):\n",
    "    return [t for t in tokens if t.isalpha() and len(t) > 2]\n",
    "\n",
    "\n",
    "# ======== 2. Stopword Removal ========\n",
    "\n",
    "def load_stopwords(file_path=\"stopwords_indo.txt\"):\n",
    "    try:\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            custom_stopwords = set(line.strip() for line in f if line.strip())\n",
    "            return set(ENGLISH_STOP_WORDS).union(custom_stopwords)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"‚ö†Ô∏è File stopwords tidak ditemukan: {file_path}. Menggunakan default ENGLISH_STOP_WORDS saja.\")\n",
    "        return set(ENGLISH_STOP_WORDS)\n",
    "\n",
    "STOPWORDS = load_stopwords()\n",
    "\n",
    "def hapus_stopword(tokens):\n",
    "    return [t for t in tokens if t not in STOPWORDS]\n",
    "\n",
    "\n",
    "# ======== 3. Pipeline Preprocessing ========\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = casefolding(text)\n",
    "    text = normalisasi(text)\n",
    "    tokens = tokenisasi(text)\n",
    "    tokens = filter_token(tokens)\n",
    "    tokens = hapus_stopword(tokens)\n",
    "    return tokens\n",
    "\n",
    "\n",
    "# ======== 4. Proses Dataset ========\n",
    "\n",
    "def preprocess_csv(file_path):\n",
    "    # perbaiki dulu CSV (buat versi _fixed)\n",
    "    fixed_path = file_path.replace(\".csv\", \"_fixed.csv\")\n",
    "    perbaiki_csv_tidak_rapi(file_path, fixed_path)\n",
    "\n",
    "    # baru baca dengan pandas\n",
    "    df = pd.read_csv(fixed_path)\n",
    "\n",
    "    # cari kolom teks utama\n",
    "    text_col = None\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype == 'object':\n",
    "            text_col = col\n",
    "            break\n",
    "\n",
    "    if text_col is None:\n",
    "        raise ValueError(f\"Tidak ada kolom teks dalam {file_path}\")\n",
    "\n",
    "    # gunakan progress bar untuk setiap teks\n",
    "    df['tokens'] = df[text_col].fillna(\"\").progress_apply(preprocess_text)\n",
    "    return df[['tokens']]\n",
    "\n",
    "\n",
    "# ======== 5. Jalankan untuk semua dataset ========\n",
    "\n",
    "os.makedirs(\"clean_dataset\", exist_ok=True)\n",
    "\n",
    "datasets = [\n",
    "    'dataset/etd_ugm_fixed.csv',\n",
    "    'dataset/etd_usk_fixed.csv',\n",
    "    'dataset/kompas_fixed.csv',\n",
    "    'dataset/mojok_fixed.csv',\n",
    "    'dataset/tempo_fixed.csv'\n",
    "]\n",
    "\n",
    "for ds in datasets:\n",
    "    try:\n",
    "        hasil = preprocess_csv(ds)\n",
    "        output_path = f\"clean_{ds}\"\n",
    "        hasil.to_csv(output_path, index=False)\n",
    "        print(f\"‚úÖ {ds} selesai diproses ‚Üí {output_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Gagal memproses {ds}: {e}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venvuts",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
